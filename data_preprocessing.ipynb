{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61b59c2-37d2-4c94-9627-4c29db8123fc",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8886cf4-271c-474e-a2c6-060bde77e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "# \n",
    "# This script processes the ATIS dataset into a unified format (general datasets which will suit for all tasks)\n",
    "# suitable for classification, tagging, generation, and prompting models.\n",
    "# Outputs:\n",
    "#  - processed/question_{train,dev,test}.jsonl    (records for classification & generation)\n",
    "#  - processed/query_{train,dev,test}.jsonl       (records for query-split classification)\n",
    "#  - processed/generation_{train,dev,test}.jsonl  (input-output pairs for seq2seq prompting)\n",
    "#  - processed/templates.json   (maps template_id -> SQL template with placeholders)\n",
    "#  - processed/tags_vocab.json  (list of all tag labels: 'O' plus placeholder names)\n",
    "#  - processed/default_values.json (default values per template for missing tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e15236d-d068-4cd7-b3ae-533768a6ac64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Preprocessing complete. Outputs are in: datasets\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "INPUT_FILE = './sources/atis.json'          # path to raw ATIS JSON\n",
    "OUTPUT_DIR = 'datasets'          # output folder for all downstream files\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Step 1: Load Raw Data ----------\n",
    "# Read the ATIS dataset into memory\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ---------- Step 2: Extract Unique SQL Templates ----------\n",
    "# We select the shortest SQL per question-group, breaking ties alphabetically\n",
    "template_to_id = OrderedDict()  # preserves insertion order\n",
    "id_to_template = {}             # inverse mapping: id -> template string\n",
    "next_template_id = 0\n",
    "\n",
    "# Container to hold one record per sentence\n",
    "records = []\n",
    "\n",
    "for entry in data:\n",
    "    sql_group = entry['sql']\n",
    "    # choose the shortest SQL; if equal length, pick lexicographically first\n",
    "    shortest_sql = min(sql_group, key=lambda s: (len(s), s))\n",
    "    # assign a unique ID to each new template\n",
    "    if shortest_sql not in template_to_id:\n",
    "        template_to_id[shortest_sql] = next_template_id\n",
    "        id_to_template[next_template_id] = shortest_sql\n",
    "        next_template_id += 1\n",
    "    tid = template_to_id[shortest_sql]\n",
    "    query_split = entry.get('query-split', '')\n",
    "\n",
    "    # process each sentence in this SQL group\n",
    "    for sent in entry['sentences']:\n",
    "        raw_text = sent['text']\n",
    "        variables = sent['variables']      # mapping placeholder -> actual value\n",
    "        question_split = sent.get('question-split', '')\n",
    "\n",
    "        # ---------- Replace placeholders in the question text ----------\n",
    "        # e.g., \"list flights to airport_code0\" -> \"list flights to DAL\"\n",
    "        filled_text = raw_text\n",
    "        for placeholder, real_val in variables.items():\n",
    "            # use re.escape to safely handle special characters\n",
    "            filled_text = re.sub(rf\"\\b{re.escape(placeholder)}\\b\", real_val, filled_text)\n",
    "\n",
    "        # ---------- Tokenize the filled text ----------\n",
    "        # simple whitespace tokenization; can be swapped for a more robust tokenizer if needed\n",
    "        tokens = filled_text.split()\n",
    "\n",
    "        # ---------- Generate BIO-style tags (here simplified as direct labels) ----------\n",
    "        # Tag each token: 'O' for outside any variable, or the placeholder name\n",
    "        tags = ['O'] * len(tokens)\n",
    "        for placeholder, real_val in variables.items():\n",
    "            val_tokens = real_val.split()\n",
    "            # sliding-window match to label multi-token values\n",
    "            for i in range(len(tokens) - len(val_tokens) + 1):\n",
    "                if tokens[i:i + len(val_tokens)] == val_tokens:\n",
    "                    for j in range(len(val_tokens)):\n",
    "                        tags[i + j] = placeholder\n",
    "\n",
    "        # ---------- Fill the SQL template with actual values ----------\n",
    "        # Prepare the fully instantiated SQL for generation and evaluation\n",
    "        sql_filled = shortest_sql\n",
    "        for placeholder, real_val in variables.items():\n",
    "            sql_filled = sql_filled.replace(placeholder, real_val)\n",
    "\n",
    "        # ---------- Assemble the record ----------\n",
    "        record = {\n",
    "            'text': filled_text,\n",
    "            'text_tokens': tokens,\n",
    "            'tags': tags,\n",
    "            'template_sql': shortest_sql,\n",
    "            'template_id': tid,\n",
    "            'sql_with_vars_filled': sql_filled,\n",
    "            'variables': variables,\n",
    "            'question_split': question_split,\n",
    "            'query_split': query_split\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "# ---------- Step 3: Build Tag Vocabulary ----------\n",
    "# Collect all unique tags across the dataset\n",
    "all_tags = set()\n",
    "for rec in records:\n",
    "    all_tags.update(rec['tags'])\n",
    "tags_vocab = sorted(all_tags)\n",
    "# Save the tag list to JSON\n",
    "with open(os.path.join(OUTPUT_DIR, 'tags_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(tags_vocab, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ---------- Step 4: Compute Default Values per Template ----------\n",
    "# For each template_id, record the first seen variable value in the TRAIN split\n",
    "default_values = {}\n",
    "for rec in records:\n",
    "    if rec['question_split'] == 'train':\n",
    "        tid = rec['template_id']\n",
    "        default_values.setdefault(str(tid), {})\n",
    "        for placeholder, real_val in rec['variables'].items():\n",
    "            if placeholder not in default_values[str(tid)]:\n",
    "                default_values[str(tid)][placeholder] = real_val\n",
    "# Save defaults to JSON (keys are strings for template_id)\n",
    "with open(os.path.join(OUTPUT_DIR, 'default_values.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(default_values, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ---------- Step 5: Write Out JSONL Splits ----------\n",
    "# Prepare writers for question-split, query-split, and generation files\n",
    "splits = ['train', 'dev', 'test']\n",
    "question_writers = {sp: open(os.path.join(OUTPUT_DIR, f'question_{sp}.jsonl'), 'w', encoding='utf-8') for sp in splits}\n",
    "query_writers    = {sp: open(os.path.join(OUTPUT_DIR, f'query_{sp}.jsonl'),   'w', encoding='utf-8') for sp in splits}\n",
    "generation_writers = {sp: open(os.path.join(OUTPUT_DIR, f'generation_{sp}.jsonl'), 'w', encoding='utf-8') for sp in splits}\n",
    "\n",
    "for rec in records:\n",
    "    qsp = rec['question_split']\n",
    "    gsp = rec['query_split']\n",
    "    # Write to question-split files (for classification & generation)\n",
    "    if qsp in question_writers:\n",
    "        question_writers[qsp].write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "    # Write to query-split files (for query-based classification)\n",
    "    if gsp in query_writers:\n",
    "        query_writers[gsp].write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "    # Write input-output pairs for generation/prompting\n",
    "    if qsp in generation_writers:\n",
    "        pair = {'input': rec['text'], 'output': rec['sql_with_vars_filled']}\n",
    "        generation_writers[qsp].write(json.dumps(pair, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Close all file handles\n",
    "for w in list(question_writers.values()) + list(query_writers.values()) + list(generation_writers.values()):\n",
    "    w.close()\n",
    "\n",
    "# ---------- Step 6: Save Templates Mapping ----------\n",
    "# Write template_id -> template SQL (with placeholders) mapping\n",
    "with open(os.path.join(OUTPUT_DIR, 'templates.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump({str(k): v for k, v in id_to_template.items()}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('✔️ Preprocessing complete. Outputs are in:', OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91b91a-0237-4c7f-b8ea-e763ecd65c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wen)",
   "language": "python",
   "name": "wen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
