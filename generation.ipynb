{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25acfd20-f5f2-4abb-8d64-4e9e5aabe344",
   "metadata": {},
   "source": [
    "### 1. Data preparation for generation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03668bb4-39c0-4c77-8577-e26421c77dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model...\n",
      "=== Start Loading Data ===\n",
      "Loading dataset from: './datasets/question_train.jsonl' and './datasets/query_train.jsonl'\n",
      "  Loaded 500 examples...\n",
      "  Loaded 1000 examples...\n",
      "  Loaded 1500 examples...\n",
      "  Loaded 2000 examples...\n",
      "  Loaded 2500 examples...\n",
      "  Loaded 3000 examples...\n",
      "  Loaded 3500 examples...\n",
      "  Loaded 4000 examples...\n",
      "Completed loading dataset. Total pairs: 4347\n",
      "\n",
      "Loading dataset from: './datasets/question_dev.jsonl' and './datasets/query_dev.jsonl'\n",
      "Completed loading dataset. Total pairs: 121\n",
      "\n",
      "Loading dataset from: './datasets/question_test.jsonl' and './datasets/question_test.jsonl'\n",
      "Completed loading dataset. Total pairs: 447\n",
      "\n",
      "Loading dataset from: './datasets/query_test.jsonl' and './datasets/query_test.jsonl'\n",
      "Completed loading dataset. Total pairs: 347\n",
      "\n",
      "=== All Data Loaded Successfully ===\n",
      "\n",
      "Vocab files exist. Loading vocabularies...\n",
      "Vocabularies loaded successfully from files.\n",
      "\n",
      "Tokenized data cached at ./cache\\train_tokenized.pkl\n",
      "Tokenized data cached at ./cache\\dev_tokenized.pkl\n",
      "Tokenized data cached at ./cache\\test_q_tokenized.pkl\n",
      "Tokenized data cached at ./cache\\test_s_tokenized.pkl\n",
      "Final Vocab Sizes:\n",
      "  Input: 884\n",
      "  Output: 550\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "# tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "# File paths\n",
    "train_question_file = \"./datasets/question_train.jsonl\"\n",
    "train_query_file    = \"./datasets/query_train.jsonl\"\n",
    "dev_question_file   = \"./datasets/question_dev.jsonl\"\n",
    "dev_query_file      = \"./datasets/query_dev.jsonl\"\n",
    "test_question_file  = \"./datasets/question_test.jsonl\"\n",
    "test_query_file     = \"./datasets/query_test.jsonl\"\n",
    "\n",
    "# compile vocab model and give vocab paths\n",
    "vocab_dir = \"./vocab\"\n",
    "input_vocab_file = os.path.join(vocab_dir, \"input_vocab.pkl\")\n",
    "output_vocab_file = os.path.join(vocab_dir, \"output_vocab.pkl\")\n",
    "os.makedirs(vocab_dir, exist_ok=True)\n",
    "\n",
    "# Cache paths, use cache to speed up, so that improve efficiency\n",
    "cache_dir = \"./cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "train_cache = os.path.join(cache_dir, 'train_tokenized.pkl')\n",
    "dev_cache = os.path.join(cache_dir, 'dev_tokenized.pkl')\n",
    "test_q_cache = os.path.join(cache_dir, 'test_q_tokenized.pkl')\n",
    "test_s_cache = os.path.join(cache_dir, 'test_s_tokenized.pkl')\n",
    "\n",
    "# Load spaCy model, \n",
    "print(\"Loading spaCy model...\")\n",
    "# use named entity recognition, lemmatiztion\n",
    "# use sm small model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"lemmatizer\"])\n",
    "\n",
    "# Dataset loader\n",
    "def load_dataset(question_path: str, query_path: str) -> List[Tuple[str, str]]:\n",
    "    print(f\"Loading dataset from: '{question_path}' and '{query_path}'\")\n",
    "    data_pairs = []\n",
    "    with open(question_path, 'r', encoding='utf-8') as fq, open(query_path, 'r', encoding='utf-8') as fs:\n",
    "        for line_num, (q_line, s_line) in enumerate(zip(fq, fs), 1):\n",
    "            q_line, s_line = q_line.strip(), s_line.strip()\n",
    "            if not q_line or not s_line:\n",
    "                continue\n",
    "            question = json.loads(q_line).get(\"text\", \"\") if q_line.startswith(\"{\") else q_line\n",
    "            sql = json.loads(s_line).get(\"sql\", \"\") if s_line.startswith(\"{\") else s_line\n",
    "            data_pairs.append((question, sql))\n",
    "            if line_num % 500 == 0:\n",
    "                print(f\"  Loaded {line_num} examples...\")\n",
    "    print(f\"Completed loading dataset. Total pairs: {len(data_pairs)}\\n\")\n",
    "    return data_pairs\n",
    "\n",
    "# spaCy tokenizers\n",
    "def tokenize_question(question: str) -> List[str]:\n",
    "    return [token.text for token in nlp(question.strip())]\n",
    "\n",
    "def tokenize_sql(sql: str) -> List[str]:\n",
    "    return [token.text for token in nlp(sql.strip())]\n",
    "\n",
    "# Load datasets\n",
    "print(\"=== Start Loading Data ===\")\n",
    "train_data = load_dataset(train_question_file, train_query_file)\n",
    "dev_data = load_dataset(dev_question_file, dev_query_file)\n",
    "test_q_data = load_dataset(test_question_file, test_question_file)\n",
    "test_s_data = load_dataset(test_query_file, test_query_file)\n",
    "print(\"=== All Data Loaded Successfully ===\\n\")\n",
    "\n",
    "# Vocab functions\n",
    "def build_vocab(train_data):\n",
    "    print(\"Building vocabulary from training data...\")\n",
    "    # use counter instance for count numbers so that we could check process at any time\n",
    "    input_counter, output_counter = Counter(), Counter()\n",
    "    for idx, (question, sql) in enumerate(train_data, 1):\n",
    "        input_counter.update(tokenize_question(question))\n",
    "        output_counter.update(tokenize_sql(sql))\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"  Processed {idx} examples for vocab building...\")\n",
    "    # pad unknow token ...\n",
    "    input_vocab = {PAD_TOKEN:0, UNK_TOKEN:1}\n",
    "    output_vocab = {PAD_TOKEN:0, UNK_TOKEN:1, SOS_TOKEN:2, EOS_TOKEN:3}\n",
    "    # koenize vocab\n",
    "    input_vocab.update({tok: len(input_vocab) for tok, _ in input_counter.items()})\n",
    "    output_vocab.update({tok: len(output_vocab) for tok, _ in output_counter.items()})\n",
    "\n",
    "    print(f\"Vocabulary built: Input vocab size {len(input_vocab)}, Output vocab size {len(output_vocab)}\\n\")\n",
    "    return input_vocab, output_vocab\n",
    "# save the mapping\n",
    "def save_vocab(input_vocab, output_vocab):\n",
    "    with open(input_vocab_file, 'wb') as f:\n",
    "        pickle.dump(input_vocab, f)\n",
    "    with open(output_vocab_file, 'wb') as f:\n",
    "        pickle.dump(output_vocab, f)\n",
    "    print(\"Vocabularies saved successfully.\\n\")\n",
    "# in this way, save a lot of time, don't need to process again\n",
    "def load_vocab():\n",
    "    with open(input_vocab_file, 'rb') as f:\n",
    "        input_vocab = pickle.load(f)\n",
    "    with open(output_vocab_file, 'rb') as f:\n",
    "        output_vocab = pickle.load(f)\n",
    "    print(\"Vocabularies loaded successfully from files.\\n\")\n",
    "    return input_vocab, output_vocab\n",
    "\n",
    "# Load or build vocabularies\n",
    "if os.path.exists(input_vocab_file) and os.path.exists(output_vocab_file):\n",
    "    print(\"Vocab files exist. Loading vocabularies...\")\n",
    "    input_vocab, output_vocab = load_vocab()\n",
    "else:\n",
    "    input_vocab, output_vocab = build_vocab(train_data)\n",
    "    save_vocab(input_vocab, output_vocab)\n",
    "\n",
    "# Cache tokenized datasets\n",
    "def preprocess_and_cache(data_pairs, cache_path):\n",
    "    tokenized_pairs = []\n",
    "    for q, s in data_pairs:\n",
    "        q_tok = [input_vocab.get(tok.text, input_vocab[UNK_TOKEN]) for tok in nlp(q.strip())]\n",
    "        s_tok = [output_vocab.get(tok.text, output_vocab[UNK_TOKEN]) for tok in nlp(s.strip())]\n",
    "        tokenized_pairs.append((q_tok, s_tok))\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(tokenized_pairs, f)\n",
    "    print(f\"Tokenized data cached at {cache_path}\")\n",
    "\n",
    "preprocess_and_cache(train_data, train_cache)\n",
    "preprocess_and_cache(dev_data, dev_cache)\n",
    "preprocess_and_cache(test_q_data, test_q_cache)\n",
    "preprocess_and_cache(test_s_data, test_s_cache)\n",
    "\n",
    "# Inverse mappings\n",
    "input_idx_to_token = {idx: tok for tok, idx in input_vocab.items()}\n",
    "output_idx_to_token = {idx: tok for tok, idx in output_vocab.items()}\n",
    "\n",
    "# Final vocab sizes\n",
    "input_vocab_size = len(input_vocab)\n",
    "output_vocab_size = len(output_vocab)\n",
    "print(f\"Final Vocab Sizes:\\n  Input: {input_vocab_size}\\n  Output: {output_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2457f1b-29df-428f-91ed-0688d0bc23a2",
   "metadata": {},
   "source": [
    "### 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb034b94-ec14-42dc-800f-add8a1f51645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached datasets...\n",
      "Epoch 1/10 - Training loss: 0.5194\n",
      "Dev Set Accuracy: 0.00%\n",
      "Epoch 2/10 - Training loss: 0.0677\n",
      "Dev Set Accuracy: 0.00%\n",
      "Epoch 3/10 - Training loss: 0.0666\n",
      "Dev Set Accuracy: 0.00%\n",
      "Epoch 4/10 - Training loss: 0.0663\n",
      "Dev Set Accuracy: 0.00%\n",
      "Epoch 5/10 - Training loss: 0.0662\n",
      "Dev Set Accuracy: 0.00%\n",
      "Epoch 6/10 - Training loss: 0.0661\n",
      "Dev Set Accuracy: 0.00%\n",
      "Epoch 7/10 - Training loss: 0.0662\n",
      "Dev Set Accuracy: 0.00%\n",
      "Epoch 8/10 - Training loss: 0.0660\n",
      "Dev Set Accuracy: 0.00%\n",
      "Epoch 9/10 - Training loss: 0.0659\n",
      "Dev Set Accuracy: 0.00%\n",
      "Epoch 10/10 - Training loss: 0.0658\n",
      "Dev Set Accuracy: 0.00%\n",
      "Question Split Test Accuracy: 0.45%\n",
      "Query Split Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# path\n",
    "cache_dir = \"./cache\"\n",
    "train_cache_file = os.path.join(cache_dir, \"train_tokenized.pkl\")\n",
    "dev_cache_file = os.path.join(cache_dir, \"dev_tokenized.pkl\")\n",
    "test_q_cache_file = os.path.join(cache_dir, \"test_q_tokenized.pkl\")\n",
    "test_s_cache_file = os.path.join(cache_dir, \"test_s_tokenized.pkl\")\n",
    "\n",
    "# Load cached tokenized data\n",
    "def load_cached_data(cache_file):\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "print(\"Loading cached datasets...\")\n",
    "train_data = load_cached_data(train_cache_file)\n",
    "dev_data = load_cached_data(dev_cache_file)\n",
    "test_q_data = load_cached_data(test_q_cache_file)\n",
    "test_s_data = load_cached_data(test_s_cache_file)\n",
    "\n",
    "# Load vocabularies\n",
    "vocab_dir = \"./vocab\"\n",
    "input_vocab = pickle.load(open(os.path.join(vocab_dir, \"input_vocab.pkl\"), 'rb'))\n",
    "output_vocab = pickle.load(open(os.path.join(vocab_dir, \"output_vocab.pkl\"), 'rb'))\n",
    "input_vocab_size = len(input_vocab)\n",
    "output_vocab_size = len(output_vocab)\n",
    "\n",
    "input_idx_to_token = {idx: tok for tok, idx in input_vocab.items()}\n",
    "output_idx_to_token = {idx: tok for tok, idx in output_vocab.items()}\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "# define seq2seq model, use torch to build model\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embed_size=128, hidden_size=256):\n",
    "        super().__init__()\n",
    "        # embedding\n",
    "        self.encoder_embed = nn.Embedding(input_vocab_size, embed_size, padding_idx=0)\n",
    "        # LSTM\n",
    "        self.encoder_lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.decoder_embed = nn.Embedding(output_vocab_size, embed_size, padding_idx=0)\n",
    "        self.decoder_lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        # use one linear for output\n",
    "        self.decoder_out = nn.Linear(hidden_size, output_vocab_size)\n",
    "\n",
    "    # for feed forward processing, use encoder \n",
    "    def forward(self, src_batch, src_lengths, tgt_batch):\n",
    "        packed_src = nn.utils.rnn.pack_padded_sequence(\n",
    "            self.encoder_embed(src_batch), src_lengths,\n",
    "            batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h, c) = self.encoder_lstm(packed_src)\n",
    "\n",
    "        # use decoder\n",
    "        outputs, _ = self.decoder_lstm(self.decoder_embed(tgt_batch), (h, c))\n",
    "        logits = self.decoder_out(outputs)\n",
    "        return logits\n",
    "\n",
    "# batchify with cached indices\n",
    "def batchify(data_pairs, batch_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        random.shuffle(data_pairs)\n",
    "\n",
    "    batches = []\n",
    "    for i in range(0, len(data_pairs), batch_size):\n",
    "        batch = data_pairs[i:i+batch_size]\n",
    "        batch_q, batch_s_input, batch_s_target = [], [], []\n",
    "        \n",
    "        # set indices\n",
    "        for q_idx, s_idx in batch:\n",
    "            s_input_idx = [output_vocab[SOS_TOKEN]] + s_idx\n",
    "            s_target_idx = s_idx + [output_vocab[EOS_TOKEN]]\n",
    "\n",
    "            batch_q.append(q_idx)\n",
    "            batch_s_input.append(s_input_idx)\n",
    "            batch_s_target.append(s_target_idx)\n",
    "            \n",
    "        # for future padding\n",
    "        q_lengths = [len(seq) for seq in batch_q]\n",
    "        max_q_len = max(q_lengths)\n",
    "        max_s_len = max(len(seq) for seq in batch_s_target)\n",
    "\n",
    "        enc_batch = [seq + [input_vocab[PAD_TOKEN]]*(max_q_len - len(seq)) for seq in batch_q]\n",
    "        dec_in_batch = [seq + [output_vocab[PAD_TOKEN]]*(max_s_len - len(seq)) for seq in batch_s_input]\n",
    "        dec_tgt_batch = [seq + [output_vocab[PAD_TOKEN]]*(max_s_len - len(seq)) for seq in batch_s_target]\n",
    "\n",
    "        batches.append((\n",
    "            torch.tensor(enc_batch, device=device),\n",
    "            q_lengths,\n",
    "            torch.tensor(dec_in_batch, device=device),\n",
    "            torch.tensor(dec_tgt_batch, device=device)\n",
    "        ))\n",
    "    return batches\n",
    "\n",
    "# training model use cross entropy loss\n",
    "def train_seq2seq(model, train_data, dev_data, epochs=10, batch_size=64, lr=0.001):\n",
    "    # use GPU\n",
    "    model.to(device)\n",
    "    # crossentropy loss ignore PAD token\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=output_vocab[PAD_TOKEN])\n",
    "    # use Adam optimizer \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        batches = batchify(train_data, batch_size)\n",
    "\n",
    "        # gradient backward process\n",
    "        for enc_batch, enc_lengths, dec_in_batch, dec_tgt_batch in batches:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(enc_batch, enc_lengths, dec_in_batch)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), dec_tgt_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(batches)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training loss: {avg_loss:.4f}\")\n",
    "\n",
    "        dev_acc = evaluate_seq2seq(model, dev_data)\n",
    "        print(f\"Dev Set Accuracy: {dev_acc*100:.2f}%\")\n",
    "\n",
    "# inference \n",
    "def infer_seq2seq(model, question_indices, max_len=100):\n",
    "    # set evaluation model\n",
    "    model.eval()\n",
    "    enc_tensor = torch.tensor([question_indices], device=device)\n",
    "    enc_len = [len(question_indices)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        packed_src = nn.utils.rnn.pack_padded_sequence(\n",
    "            model.encoder_embed(enc_tensor), enc_len, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h, c) = model.encoder_lstm(packed_src)\n",
    "\n",
    "        dec_input = torch.tensor([[output_vocab[SOS_TOKEN]]], device=device)\n",
    "        pred_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, (h, c) = model.decoder_lstm(model.decoder_embed(dec_input), (h, c))\n",
    "            logits = model.decoder_out(output.squeeze(1))\n",
    "            pred_idx = logits.argmax(1).item()\n",
    "            if pred_idx == output_vocab[EOS_TOKEN]:\n",
    "                break\n",
    "            pred_tokens.append(output_idx_to_token.get(pred_idx, UNK_TOKEN))\n",
    "            dec_input = torch.tensor([[pred_idx]], device=device)\n",
    "\n",
    "    return \" \".join(pred_tokens)\n",
    "\n",
    "def evaluate_seq2seq(model, dataset):\n",
    "    correct = 0\n",
    "    for q_idx, sql_idx in dataset:\n",
    "        pred_sql = infer_seq2seq(model, q_idx)\n",
    "        gold_sql = \" \".join([output_idx_to_token[idx] for idx in sql_idx])\n",
    "        if pred_sql.strip().lower() == gold_sql.strip().lower():\n",
    "            correct += 1\n",
    "    return correct / len(dataset)\n",
    "\n",
    "lstm_model = Seq2SeqModel(input_vocab_size, output_vocab_size)\n",
    "train_seq2seq(lstm_model, train_data, dev_data, epochs=10, batch_size=64)\n",
    "\n",
    "acc_question = evaluate_seq2seq(lstm_model, test_q_data)\n",
    "acc_query = evaluate_seq2seq(lstm_model, test_s_data)\n",
    "print(f\"Question Split Test Accuracy: {acc_question*100:.2f}%\")\n",
    "print(f\"Query Split Test Accuracy: {acc_query*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda06094-335a-44fd-84a5-f4164f8e4b2a",
   "metadata": {},
   "source": [
    "### 3. LSTM Encoder-Decoder with Attentiony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818384cd-5323-4818-b249-e0baf34ac5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached datasets...\n",
      "Epoch 1/10 - Loss: 0.3770\n",
      "Dev Accuracy: 0.00%\n",
      "Epoch 2/10 - Loss: 0.0670\n",
      "Dev Accuracy: 0.00%\n",
      "Epoch 3/10 - Loss: 0.0663\n",
      "Dev Accuracy: 0.00%\n",
      "Epoch 4/10 - Loss: 0.0662\n",
      "Dev Accuracy: 0.00%\n",
      "Epoch 5/10 - Loss: 0.0660\n",
      "Dev Accuracy: 0.00%\n",
      "Epoch 6/10 - Loss: 0.0658\n",
      "Dev Accuracy: 0.00%\n",
      "Epoch 7/10 - Loss: 0.0660\n",
      "Dev Accuracy: 0.00%\n",
      "Epoch 8/10 - Loss: 0.0659\n",
      "Dev Accuracy: 0.00%\n",
      "Epoch 9/10 - Loss: 0.0658\n",
      "Dev Accuracy: 0.00%\n",
      "Epoch 10/10 - Loss: 0.0655\n",
      "Dev Accuracy: 0.00%\n",
      "Question Split Accuracy: 0.45%\n",
      "Query Split Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load cached datasets\n",
    "cache_dir = \"./cache\"\n",
    "\n",
    "def load_cached_data(filename):\n",
    "    path = os.path.join(cache_dir, filename)\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "print(\"Loading cached datasets...\")\n",
    "train_data = load_cached_data(\"train_tokenized.pkl\")\n",
    "dev_data = load_cached_data(\"dev_tokenized.pkl\")\n",
    "test_q_data = load_cached_data(\"test_q_tokenized.pkl\")\n",
    "test_s_data = load_cached_data(\"test_s_tokenized.pkl\")\n",
    "\n",
    "# Load vocabularies\n",
    "vocab_dir = \"./vocab\"\n",
    "with open(os.path.join(vocab_dir, \"input_vocab.pkl\"), 'rb') as f:\n",
    "    input_vocab = pickle.load(f)\n",
    "with open(os.path.join(vocab_dir, \"output_vocab.pkl\"), 'rb') as f:\n",
    "    output_vocab = pickle.load(f)\n",
    "\n",
    "input_vocab_size = len(input_vocab)\n",
    "output_vocab_size = len(output_vocab)\n",
    "\n",
    "input_idx_to_token = {idx: tok for tok, idx in input_vocab.items()}\n",
    "output_idx_to_token = {idx: tok for tok, idx in output_vocab.items()}\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "class Seq2SeqAttnModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embed_size=128, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_embed = nn.Embedding(input_vocab_size, embed_size, padding_idx=input_vocab[PAD_TOKEN])\n",
    "        self.encoder_lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_embed = nn.Embedding(output_vocab_size, embed_size, padding_idx=output_vocab[PAD_TOKEN])\n",
    "        self.decoder_lstm = nn.LSTM(embed_size + hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Attention layers\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.attn_v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "        # Output layer\n",
    "        self.out_linear = nn.Linear(hidden_size * 2, output_vocab_size)\n",
    "\n",
    "    def forward(self, src_batch, src_lengths, tgt_batch):\n",
    "        src_emb = self.encoder_embed(src_batch)\n",
    "        packed_src = nn.utils.rnn.pack_padded_sequence(src_emb, src_lengths, batch_first=True, enforce_sorted=False)\n",
    "        enc_outputs, (h_enc, c_enc) = self.encoder_lstm(packed_src)\n",
    "        enc_outputs, _ = nn.utils.rnn.pad_packed_sequence(enc_outputs, batch_first=True)\n",
    "\n",
    "        h_dec, c_dec = h_enc, c_enc\n",
    "        logits = []\n",
    "\n",
    "        dec_embedded = self.decoder_embed(tgt_batch)\n",
    "\n",
    "        for t in range(tgt_batch.size(1)):\n",
    "            dec_input_t = dec_embedded[:, t, :].unsqueeze(1)\n",
    "\n",
    "            attn_weights = self.calculate_attention(h_dec[-1], enc_outputs, src_lengths)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs)\n",
    "\n",
    "            dec_input_combined = torch.cat([dec_input_t, context], dim=2)\n",
    "            output, (h_dec, c_dec) = self.decoder_lstm(dec_input_combined, (h_dec, c_dec))\n",
    "\n",
    "            output_combined = torch.cat([output, context], dim=2).squeeze(1)\n",
    "            logits.append(self.out_linear(output_combined).unsqueeze(1))\n",
    "\n",
    "        logits = torch.cat(logits, dim=1)\n",
    "        return logits\n",
    "\n",
    "    # use tanh for activation function \n",
    "    def calculate_attention(self, hidden, encoder_outputs, src_lengths):\n",
    "        hidden_expanded = hidden.unsqueeze(1).expand_as(encoder_outputs)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden_expanded, encoder_outputs), dim=2)))\n",
    "        scores = self.attn_v(energy).squeeze(2)\n",
    "\n",
    "        mask = torch.arange(encoder_outputs.size(1), device=device).unsqueeze(0) >= torch.tensor(src_lengths, device=device).unsqueeze(1)\n",
    "        scores.data.masked_fill_(mask, -float('inf'))\n",
    "        return torch.softmax(scores, dim=1)\n",
    "\n",
    "def batchify(data_pairs, batch_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        random.shuffle(data_pairs)\n",
    "    batches = []\n",
    "    for i in range(0, len(data_pairs), batch_size):\n",
    "        batch = data_pairs[i:i+batch_size]\n",
    "        batch_q, batch_s_input, batch_s_target = [], [], []\n",
    "        for q_idx, s_idx in batch:\n",
    "            s_input_idx = [output_vocab[SOS_TOKEN]] + s_idx\n",
    "            s_target_idx = s_idx + [output_vocab[EOS_TOKEN]]\n",
    "\n",
    "            batch_q.append(q_idx)\n",
    "            batch_s_input.append(s_input_idx)\n",
    "            batch_s_target.append(s_target_idx)\n",
    "\n",
    "        q_lengths = [len(q) for q in batch_q]\n",
    "        max_q_len = max(q_lengths)\n",
    "        max_s_len = max(len(s) for s in batch_s_target)\n",
    "\n",
    "        enc_batch = [seq + [input_vocab[PAD_TOKEN]]*(max_q_len-len(seq)) for seq in batch_q]\n",
    "        dec_in_batch = [seq + [output_vocab[PAD_TOKEN]]*(max_s_len-len(seq)) for seq in batch_s_input]\n",
    "        dec_tgt_batch = [seq + [output_vocab[PAD_TOKEN]]*(max_s_len-len(seq)) for seq in batch_s_target]\n",
    "\n",
    "        batches.append((torch.tensor(enc_batch, device=device), q_lengths,\n",
    "                        torch.tensor(dec_in_batch, device=device),\n",
    "                        torch.tensor(dec_tgt_batch, device=device)))\n",
    "    return batches\n",
    "\n",
    "# we need to normalize the sql\n",
    "def normalize_sql(sql):\n",
    "    return ' '.join(sql.lower().strip().rstrip(';').split())\n",
    "\n",
    "def train_seq2seq_attn(model, train_data, dev_data=None, epochs=10, batch_size=64, lr=0.001):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=output_vocab[PAD_TOKEN])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        batches = batchify(train_data, batch_size)\n",
    "\n",
    "        for enc_batch, q_lengths, dec_input_batch, dec_target_batch in batches:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(enc_batch, q_lengths, dec_input_batch)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), dec_target_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(batches)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if dev_data:\n",
    "            dev_acc = evaluate_seq2seq_attn(model, dev_data)\n",
    "            print(f\"Dev Accuracy: {dev_acc*100:.2f}%\")\n",
    "\n",
    "# inference using token\n",
    "def infer_seq2seq_attn(model, question_indices, max_len=100):\n",
    "    model.eval()\n",
    "    enc_tensor = torch.tensor([question_indices], device=device)\n",
    "    enc_len = [len(question_indices)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.encoder_embed(enc_tensor)\n",
    "        packed_src = nn.utils.rnn.pack_padded_sequence(src_emb, enc_len, batch_first=True, enforce_sorted=False)\n",
    "        enc_outputs, (h_dec, c_dec) = model.encoder_lstm(packed_src)\n",
    "        enc_outputs, _ = nn.utils.rnn.pad_packed_sequence(enc_outputs, batch_first=True)\n",
    "\n",
    "        dec_input_idx = output_vocab[SOS_TOKEN]\n",
    "        pred_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            dec_emb = model.decoder_embed(torch.tensor([[dec_input_idx]], device=device))\n",
    "            attn_weights = model.calculate_attention(h_dec[-1], enc_outputs, enc_len)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs)\n",
    "            dec_input_combined = torch.cat([dec_emb, context], dim=2)\n",
    "            output, (h_dec, c_dec) = model.decoder_lstm(dec_input_combined, (h_dec, c_dec))\n",
    "            logits = model.out_linear(torch.cat([output, context], dim=2).squeeze(1))\n",
    "            pred_idx = logits.argmax(-1).item()\n",
    "            if pred_idx == output_vocab[EOS_TOKEN]:\n",
    "                break\n",
    "            pred_tokens.append(output_idx_to_token.get(pred_idx, UNK_TOKEN))\n",
    "            dec_input_idx = pred_idx\n",
    "\n",
    "    return \" \".join(pred_tokens)\n",
    "\n",
    "def evaluate_seq2seq_attn(model, dataset):\n",
    "    correct = 0\n",
    "    for q_indices, sql_indices in dataset:\n",
    "        pred_sql = infer_seq2seq_attn(model, q_indices)\n",
    "        gold_sql = \" \".join([output_idx_to_token[idx] for idx in sql_indices])\n",
    "        if normalize_sql(pred_sql) == normalize_sql(gold_sql):\n",
    "            correct += 1\n",
    "    return correct / len(dataset)\n",
    "\n",
    "attn_model = Seq2SeqAttnModel(input_vocab_size, output_vocab_size)\n",
    "train_seq2seq_attn(attn_model, train_data, dev_data, epochs=10, batch_size=64, lr=0.001)\n",
    "\n",
    "acc_question = evaluate_seq2seq_attn(attn_model, test_q_data)\n",
    "acc_query = evaluate_seq2seq_attn(attn_model, test_s_data)\n",
    "print(f\"Question Split Accuracy: {acc_question*100:.2f}%\")\n",
    "print(f\"Query Split Accuracy: {acc_query*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8a472-0529-47cf-8c03-06f20a9d4642",
   "metadata": {},
   "source": [
    "### 4. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b066c9-0fb1-4ec2-a80d-63585ba385e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5655\n",
      "Epoch 2/10, Loss: 0.0832\n",
      "Epoch 3/10, Loss: 0.0768\n",
      "Epoch 4/10, Loss: 0.0748\n",
      "Epoch 5/10, Loss: 0.0736\n",
      "Epoch 6/10, Loss: 0.0707\n",
      "Epoch 7/10, Loss: 0.0670\n",
      "Epoch 8/10, Loss: 0.0634\n",
      "Epoch 9/10, Loss: 0.0627\n",
      "Epoch 10/10, Loss: 0.0609\n",
      "Transformer - Question Split Test Accuracy: 0.45%\n",
      "Transformer - Query Split Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Directory paths\n",
    "cache_dir = \"./cache\"\n",
    "vocab_dir = \"./vocab\"\n",
    "\n",
    "# Load cached data\n",
    "def load_cached_data(filename):\n",
    "    path = os.path.join(cache_dir, filename)\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "train_data = load_cached_data(\"train_tokenized.pkl\")\n",
    "dev_data = load_cached_data(\"dev_tokenized.pkl\")\n",
    "test_q_data = load_cached_data(\"test_q_tokenized.pkl\")\n",
    "test_s_data = load_cached_data(\"test_s_tokenized.pkl\")\n",
    "\n",
    "# Load vocabularies\n",
    "with open(os.path.join(vocab_dir, \"input_vocab.pkl\"), 'rb') as f:\n",
    "    input_vocab = pickle.load(f)\n",
    "with open(os.path.join(vocab_dir, \"output_vocab.pkl\"), 'rb') as f:\n",
    "    output_vocab = pickle.load(f)\n",
    "\n",
    "input_vocab_size = len(input_vocab)\n",
    "output_vocab_size = len(output_vocab)\n",
    "input_idx_to_token = {idx: tok for tok, idx in input_vocab.items()}\n",
    "output_idx_to_token = {idx: tok for tok, idx in output_vocab.items()}\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embed_size=128, num_heads=8, num_encoder_layers=3,\n",
    "                 num_decoder_layers=3, ff_hidden_size=512, dropout=0.1, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, embed_size, padding_idx=input_vocab[PAD_TOKEN])\n",
    "        self.output_embedding = nn.Embedding(output_vocab_size, embed_size, padding_idx=output_vocab[PAD_TOKEN])\n",
    "\n",
    "        self.positional_encoding = nn.Parameter(self._generate_positional_encoding(max_seq_len, embed_size), requires_grad=False)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=ff_hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask, tgt_mask):\n",
    "        src_emb = self.input_embedding(src) + self.positional_encoding[:src.size(1), :]\n",
    "        tgt_emb = self.output_embedding(tgt) + self.positional_encoding[:tgt.size(1), :]\n",
    "\n",
    "        src_emb = src_emb.transpose(0, 1)\n",
    "        tgt_emb = tgt_emb.transpose(0, 1)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src_emb, tgt_emb,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_key_padding_mask=src_padding_mask\n",
    "        )\n",
    "\n",
    "        output = output.transpose(0, 1)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_len, d_model):\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "# Generate masks\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
    "    return mask\n",
    "\n",
    "def create_padding_mask(seq, pad_idx):\n",
    "    return (seq == pad_idx)\n",
    "\n",
    "# Batchify\n",
    "def batchify(data_pairs, batch_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        random.shuffle(data_pairs)\n",
    "    batches = []\n",
    "    for i in range(0, len(data_pairs), batch_size):\n",
    "        batch = data_pairs[i:i + batch_size]\n",
    "        batch_q, batch_s_input, batch_s_target = [], [], []\n",
    "        for q, s in batch:\n",
    "            s_input_idx = [output_vocab[SOS_TOKEN]] + s\n",
    "            s_target_idx = s + [output_vocab[EOS_TOKEN]]\n",
    "            batch_q.append(q)\n",
    "            batch_s_input.append(s_input_idx)\n",
    "            batch_s_target.append(s_target_idx)\n",
    "\n",
    "        max_q_len = max(len(q) for q in batch_q)\n",
    "        max_s_len = max(len(s) for s in batch_s_input)\n",
    "\n",
    "        enc_batch = [seq + [input_vocab[PAD_TOKEN]] * (max_q_len - len(seq)) for seq in batch_q]\n",
    "        dec_in_batch = [seq + [output_vocab[PAD_TOKEN]] * (max_s_len - len(seq)) for seq in batch_s_input]\n",
    "        dec_tgt_batch = [seq + [output_vocab[PAD_TOKEN]] * (max_s_len - len(seq)) for seq in batch_s_target]\n",
    "\n",
    "        batches.append((\n",
    "            torch.tensor(enc_batch, device=device),\n",
    "            torch.tensor(dec_in_batch, device=device),\n",
    "            torch.tensor(dec_tgt_batch, device=device)\n",
    "        ))\n",
    "    return batches\n",
    "\n",
    "# Train function (with masks)\n",
    "def train(model, train_data, dev_data=None, epochs=10, batch_size=64, lr=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=output_vocab[PAD_TOKEN])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batches = batchify(train_data, batch_size)\n",
    "\n",
    "        for src, tgt_input, tgt_output in batches:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1))\n",
    "            src_padding_mask = create_padding_mask(src, input_vocab[PAD_TOKEN])\n",
    "            tgt_padding_mask = create_padding_mask(tgt_input, output_vocab[PAD_TOKEN])\n",
    "\n",
    "            output = model(src, tgt_input, src_padding_mask, tgt_padding_mask, tgt_mask)\n",
    "            loss = criterion(output.view(-1, output_vocab_size), tgt_output.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(batches)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# SQL normalization function\n",
    "def normalize_sql(sql_str):\n",
    "    return ' '.join(sql_str.lower().strip().rstrip(';').split())\n",
    "\n",
    "# Inference function\n",
    "def infer(model, src_sequence, max_len=100):\n",
    "    model.eval()\n",
    "    src_tensor = torch.tensor([src_sequence], device=device)\n",
    "    generated = [output_vocab[SOS_TOKEN]]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            tgt_tensor = torch.tensor([generated], device=device)\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt_tensor.size(1))\n",
    "            src_padding_mask = create_padding_mask(src_tensor, input_vocab[PAD_TOKEN])\n",
    "            tgt_padding_mask = create_padding_mask(tgt_tensor, output_vocab[PAD_TOKEN])\n",
    "\n",
    "            output = model(src_tensor, tgt_tensor, src_padding_mask, tgt_padding_mask, tgt_mask)\n",
    "            pred_token = output.argmax(-1)[:, -1].item()\n",
    "            if pred_token == output_vocab[EOS_TOKEN]:\n",
    "                break\n",
    "            generated.append(pred_token)\n",
    "\n",
    "    return \" \".join(output_idx_to_token.get(idx, UNK_TOKEN) for idx in generated[1:])\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataset):\n",
    "    correct = 0\n",
    "    for q_tokens, sql_tokens in dataset:\n",
    "        pred_sql = infer(model, q_tokens)\n",
    "        gold_sql = \" \".join(output_idx_to_token.get(idx, UNK_TOKEN) for idx in sql_tokens)\n",
    "        if normalize_sql(pred_sql) == normalize_sql(gold_sql):\n",
    "            correct += 1\n",
    "    return correct / len(dataset)\n",
    "\n",
    "# Initialize, train, and evaluate the model\n",
    "transformer_model = TransformerModel(input_vocab_size, output_vocab_size)\n",
    "train(transformer_model, train_data, dev_data, epochs=10, batch_size=128, lr=1e-3)\n",
    "\n",
    "acc_question = evaluate(transformer_model, test_q_data)\n",
    "acc_query = evaluate(transformer_model, test_s_data)\n",
    "print(f\"Transformer - Question Split Test Accuracy: {acc_question*100:.2f}%\")\n",
    "print(f\"Transformer - Query Split Test Accuracy: {acc_query*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wen)",
   "language": "python",
   "name": "wen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
