{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de5d691-baac-4ec5-b80c-ecb37c8f9194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Preprocessing done. All outputs in: processed\n"
     ]
    }
   ],
   "source": [
    "# preprocess.py\n",
    "# \n",
    "# This script processes the ATIS dataset (atis.json) into a unified format\n",
    "# suitable for classification, tagging, generation, and prompting models.\n",
    "# Outputs:\n",
    "#  - question_train.jsonl, question_dev.jsonl, question_test.jsonl\n",
    "#  - query_train.jsonl,   query_dev.jsonl,   query_test.jsonl\n",
    "#  - templates.json         # maps template_id -> SQL template (placeholders)\n",
    "#  - tags_vocab.json        # list of all tag types (\"O\" + variable names)\n",
    "#  - default_values.json    # default placeholder values per template for missed tags\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "INPUT_FILE = 'atis.json'          # path to raw ATIS JSON\n",
    "OUTPUT_DIR = 'processed'          # output folder for all downstream files\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Load Raw Data ----------\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ---------- Step 1: Extract Unique SQL Templates ----------\n",
    "# We select the shortest SQL per question-group, tie-breaking lexicographically.\n",
    "template_to_id = OrderedDict()\n",
    "id_to_template = {}\n",
    "next_template_id = 0\n",
    "\n",
    "action_records = []  # collects processed sentence-level records\n",
    "\n",
    "for entry in data:\n",
    "    sql_group = entry['sql']\n",
    "    # pick shortest SQL (then alphabetical)\n",
    "    shortest_sql = min(sql_group, key=lambda s: (len(s), s))\n",
    "    if shortest_sql not in template_to_id:\n",
    "        template_to_id[shortest_sql] = next_template_id\n",
    "        id_to_template[next_template_id] = shortest_sql\n",
    "        next_template_id += 1\n",
    "    tid = template_to_id[shortest_sql]\n",
    "    query_split = entry.get('query-split', '')\n",
    "\n",
    "    # process each sentence in the group\n",
    "    for sent in entry['sentences']:\n",
    "        raw_text = sent['text']\n",
    "        variables = sent['variables']        # placeholder -> actual value\n",
    "        question_split = sent.get('question-split', '')\n",
    "\n",
    "        # ---------- Step 2: Replace placeholders in text ----------\n",
    "        filled_text = raw_text\n",
    "        for placeholder, real_val in variables.items():\n",
    "            # word-boundary replace, e.g. city_name0 -> BOSTON\n",
    "            filled_text = re.sub(rf\"\\b{placeholder}\\b\", real_val, filled_text)\n",
    "\n",
    "        # ---------- Step 3: Tokenize ----------\n",
    "        tokens = filled_text.split()  # simple whitespace tokenizer\n",
    "\n",
    "        # ---------- Step 4: Generate Tag Sequence ----------\n",
    "        # Tag each token either 'O' or the variable placeholder name\n",
    "        tags = ['O'] * len(tokens)\n",
    "        for placeholder, real_val in variables.items():\n",
    "            val_tokens = real_val.split()\n",
    "            if not val_tokens:\n",
    "                continue\n",
    "            # sliding-window match for multi-token values\n",
    "            for i in range(len(tokens) - len(val_tokens) + 1):\n",
    "                if tokens[i:i+len(val_tokens)] == val_tokens:\n",
    "                    for j in range(len(val_tokens)):\n",
    "                        tags[i+j] = placeholder\n",
    "\n",
    "        # ---------- Step 5: Fill SQL Template ----------\n",
    "        sql_template = shortest_sql\n",
    "        sql_filled = sql_template\n",
    "        for placeholder, real_val in variables.items():\n",
    "            # replace placeholder in SQL, e.g. \"city_name0\" -> \"BOSTON\"\n",
    "            sql_filled = sql_filled.replace(placeholder, real_val)\n",
    "\n",
    "        # ---------- Collect Record ----------\n",
    "        record = {\n",
    "            'text': filled_text,\n",
    "            'text_tokens': tokens,\n",
    "            'tags': tags,\n",
    "            'template_sql': sql_template,\n",
    "            'template_id': tid,\n",
    "            'sql_with_vars_filled': sql_filled,\n",
    "            'variables': variables,\n",
    "            'question_split': question_split,\n",
    "            'query_split': query_split\n",
    "        }\n",
    "        action_records.append(record)\n",
    "\n",
    "# ---------- Step 6: Build Tag Vocabulary ----------\n",
    "all_tags = set()\n",
    "for rec in action_records:\n",
    "    all_tags.update(rec['tags'])\n",
    "tags_vocab = sorted(all_tags)  # e.g. ['O', 'airport_code0', 'city_name0', ...]\n",
    "with open(os.path.join(OUTPUT_DIR, 'tags_vocab.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(tags_vocab, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ---------- Step 7: Compute Default Values per Template ----------\n",
    "# For each template_id, choose the first seen value for each placeholder in TRAIN set\n",
    "default_values = defaultdict(dict)\n",
    "for rec in action_records:\n",
    "    if rec['question_split'] == 'train':\n",
    "        tid = rec['template_id']\n",
    "        for placeholder, real_val in rec['variables'].items():\n",
    "            if placeholder not in default_values[tid]:\n",
    "                default_values[tid][placeholder] = real_val\n",
    "# Save defaults\n",
    "with open(os.path.join(OUTPUT_DIR, 'default_values.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(default_values, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ---------- Step 8: Write Out JSONL Splits ----------\n",
    "splits = ['train', 'dev', 'test']\n",
    "writers_q = {sp: open(os.path.join(OUTPUT_DIR, f'question_{sp}.jsonl'), 'w', encoding='utf-8') for sp in splits}\n",
    "writers_g = {sp: open(os.path.join(OUTPUT_DIR, f'query_{sp}.jsonl'), 'w', encoding='utf-8') for sp in splits}\n",
    "\n",
    "for rec in action_records:\n",
    "    qsp = rec['question_split']\n",
    "    gsp = rec['query_split']\n",
    "    if qsp in writers_q:\n",
    "        writers_q[qsp].write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "    if gsp in writers_g:\n",
    "        writers_g[gsp].write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# close file handles\n",
    "for f in writers_q.values(): f.close()\n",
    "for f in writers_g.values(): f.close()\n",
    "\n",
    "# ---------- Step 9: Save Templates Mapping ----------\n",
    "with open(os.path.join(OUTPUT_DIR, 'templates.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(id_to_template, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('✔️ Preprocessing done. All outputs in:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787527e9-2fca-455c-9f05-064cbd3afbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
