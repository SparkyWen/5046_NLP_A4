{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5070e0-a10e-49d1-8edb-b8e56bf773eb",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df34df6-a61b-4850-a330-76860ed72c23",
   "metadata": {},
   "source": [
    "## 1. load_data & build_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2692e3a-7661-45d9-8fd2-e1df5e11f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract 825 unque templates。\n",
      "\n",
      "templatesID: 0\n",
      "templatesSQL: SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT AS AIRPORTalias0 , FLIGHT AS FLIGHTalias0 WHERE AIRPORTalias0.AIRPORT_CODE = \"airport_code0\" AND FLIGHTalias0.TO_AIRPORT = AIRPORTalias0.AIRPORT_CODE ;\n",
      "default values of variables: {'airport_code0': 'MKE'}\n",
      "\n",
      "templatesID: 1\n",
      "templatesSQL: SELECT DISTINCT FLIGHTalias0.FLIGHT_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , DATE_DAY AS DATE_DAYalias0 , DAYS AS DAYSalias0 , FLIGHT AS FLIGHTalias0 WHERE ( CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"city_name1\" AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"city_name0\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND DATE_DAYalias0.DAY_NUMBER = day_number0 AND DATE_DAYalias0.MONTH_NUMBER = month_number0 AND DATE_DAYalias0.YEAR = year0 AND DAYSalias0.DAY_NAME = DATE_DAYalias0.DAY_NAME AND FLIGHTalias0.FLIGHT_DAYS = DAYSalias0.DAYS_CODE ;\n",
      "default values of variables: {'city_name1': 'DENVER', 'month_number0': '8', 'day_number0': '9', 'city_name0': 'BOSTON'}\n",
      "\n",
      "templatesID: 2\n",
      "templatesSQL: SELECT DISTINCT FAREalias0.FARE_ID FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias0 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias1 , CITY AS CITYalias0 , CITY AS CITYalias1 , FARE AS FAREalias0 , FLIGHT AS FLIGHTalias0 , FLIGHT_FARE AS FLIGHT_FAREalias0 WHERE ( CITYalias0.CITY_CODE = AIRPORT_SERVICEalias0.CITY_CODE AND CITYalias0.CITY_NAME = \"city_name0\" AND CITYalias1.CITY_CODE = AIRPORT_SERVICEalias1.CITY_CODE AND CITYalias1.CITY_NAME = \"city_name1\" AND FLIGHTalias0.FROM_AIRPORT = AIRPORT_SERVICEalias0.AIRPORT_CODE AND FLIGHTalias0.TO_AIRPORT = AIRPORT_SERVICEalias1.AIRPORT_CODE ) AND FAREalias0.ONE_DIRECTION_COST = ( SELECT MAX( FAREalias1.ONE_DIRECTION_COST ) FROM AIRPORT_SERVICE AS AIRPORT_SERVICEalias2 , AIRPORT_SERVICE AS AIRPORT_SERVICEalias3 , CITY AS CITYalias2 , CITY AS CITYalias3 , FARE AS FAREalias1 , FLIGHT AS FLIGHTalias1 , FLIGHT_FARE AS FLIGHT_FAREalias1 WHERE ( CITYalias2.CITY_CODE = AIRPORT_SERVICEalias2.CITY_CODE AND CITYalias2.CITY_NAME = \"city_name0\" AND CITYalias3.CITY_CODE = AIRPORT_SERVICEalias3.CITY_CODE AND CITYalias3.CITY_NAME = \"city_name1\" AND FLIGHTalias1.FROM_AIRPORT = AIRPORT_SERVICEalias2.AIRPORT_CODE AND FLIGHTalias1.TO_AIRPORT = AIRPORT_SERVICEalias3.AIRPORT_CODE ) AND FAREalias1.ROUND_TRIP_REQUIRED = \"round_trip_required0\" AND FLIGHT_FAREalias1.FARE_ID = FAREalias1.FARE_ID AND FLIGHTalias1.AIRLINE_CODE = \"airline_code0\" AND FLIGHTalias1.FLIGHT_ID = FLIGHT_FAREalias1.FLIGHT_ID ) AND FLIGHT_FAREalias0.FARE_ID = FAREalias0.FARE_ID AND FLIGHTalias0.AIRLINE_CODE = \"airline_code0\" AND FLIGHTalias0.FLIGHT_ID = FLIGHT_FAREalias0.FLIGHT_ID ;\n",
      "default values of variables: {'city_name0': 'BOSTON', 'city_name1': 'ATLANTA', 'airline_code0': 'AA'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"load\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def pick_shortest_sql(sql_list):\n",
    "    \"\"\"choose shortest, if tie, choose Alphabetically\"\"\"\n",
    "    queries = [q.strip() for q in sql_list if q.strip()]\n",
    "    min_len = min(len(q) for q in queries)\n",
    "    shortest_queries = [q for q in queries if len(q) == min_len]\n",
    "    shortest_queries.sort()\n",
    "    return shortest_queries[0]\n",
    "\n",
    "def build_templates(data):\n",
    "    \"\"\"build dictionary and deault dic\"\"\"\n",
    "    template_id_map = {}\n",
    "    templates = {}\n",
    "    default_values = {}\n",
    "    current_id = 0\n",
    "\n",
    "    # i check the rubric, it says maybe there is a circumstance that template_id is new in dev/test, just skip it.\n",
    "    # that why we only need to take training datasets into consideration.\n",
    "    for entry in data:\n",
    "        if entry.get(\"query-split\") != \"train\":\n",
    "            continue\n",
    "\n",
    "        shortest_query = pick_shortest_sql(entry[\"sql\"])\n",
    "        if not shortest_query:\n",
    "            continue\n",
    "\n",
    "        if shortest_query not in template_id_map:\n",
    "            template_id_map[shortest_query] = current_id\n",
    "            templates[current_id] = shortest_query\n",
    "            default_val_dict = {}\n",
    "\n",
    "            # extract default variable values ​​for each template and ensure that the order in which the variable values ​​are extracted is consistent \n",
    "            # with the order in which the variables appear in the original sentence.\n",
    "            # that means if change the order, it won't have negative impact on the results\n",
    "            for sent in entry.get(\"sentences\", []):\n",
    "                if sent.get(\"question-split\") == \"train\":\n",
    "                    text = sent[\"text\"]\n",
    "                    variables = sent[\"variables\"]\n",
    "                    placeholder_spans = sorted(\n",
    "                        ((match.start(), var_name)\n",
    "                         for var_name in variables\n",
    "                         for match in re.finditer(re.escape(var_name), text)),\n",
    "                        key=lambda x: x[0]\n",
    "                    )\n",
    "                    ordered_vars = [var_name for _, var_name in placeholder_spans]\n",
    "                    for var in ordered_vars:\n",
    "                        default_val_dict[var] = variables[var]\n",
    "                    break\n",
    "\n",
    "            default_values[current_id] = default_val_dict\n",
    "            current_id += 1\n",
    "\n",
    "    return templates, default_values\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_data(\"./sources/atis.json\")\n",
    "    templates, default_values = build_templates(data)\n",
    "    template_sql_to_id = {sql: tid for tid, sql in templates.items()}\n",
    "\n",
    "    print(f\"extract {len(templates)} unque templates。\")\n",
    "    for tid in list(templates)[:3]:\n",
    "        print(f\"\\ntemplatesID: {tid}\")\n",
    "        print(\"templatesSQL:\", templates[tid])\n",
    "        print(\"default values of variables:\", default_values[tid])\n",
    "\n",
    "    with open(\"./datasets/templates.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(templates, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(\"./datasets/default_values.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(default_values, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(\"./datasets/template_sql_to_id.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(template_sql_to_id, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac4bd5-7d9b-401c-9cde-0666d8b2362e",
   "metadata": {},
   "source": [
    "## 2. build_tags_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d4459d-50ce-4ba4-b56c-7fbde15fe2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tags（include'O'）：61\n",
      "examples: ['O', 'aircraft_code0', 'airline_code0', 'airline_code1', 'airline_code2', 'airline_name0', 'airport_code0', 'airport_code1', 'airport_name0', 'arrival_time0']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def build_tags_vocab(data):\n",
    "    \"\"\"build tags\"\"\"\n",
    "    tags_set = {\"O\"}\n",
    "    for entry in data:\n",
    "        for sent in entry.get(\"sentences\", []):\n",
    "            if sent.get(\"question-split\") == \"train\":\n",
    "                tags_set.update(sent.get(\"variables\", {}).keys())\n",
    "    return sorted(tags_set)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_data(\"./sources/atis.json\")\n",
    "    tags_vocab = build_tags_vocab(data)\n",
    "    print(f\"total tags（include'O'）：{len(tags_vocab)}\")\n",
    "    print(\"examples:\", tags_vocab[:10])\n",
    "\n",
    "    with open(\"./datasets/tags_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(tags_vocab, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33b616-cb8a-4762-bb78-ccfafe21feab",
   "metadata": {},
   "source": [
    "## 3. replace_placeholders & process_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746d7bbe-e8c0-4a10-9311-24ccc7d507bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "nlp = spacy.blank(\"en\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Replace the placeholders (such as city_name0, airport_code0) with the actual variable values ​​(such as DALLAS, MKE), \n",
    "# and make sure the replacement order is consistent with the order in which the placeholders appear in the original sentence.\n",
    "# same to the module 2 to avoid the order problems\n",
    "def replace_placeholders(text, variables):\n",
    "    placeholder_spans = sorted(\n",
    "        ((match.start(), var)\n",
    "         for var in variables\n",
    "         for match in re.finditer(r'\\b'+re.escape(var)+r'\\b', text)),\n",
    "        key=lambda x: x[0]\n",
    "    )\n",
    "    new_text = text\n",
    "    for _, var in placeholder_spans:\n",
    "        val = variables[var]\n",
    "        new_text = re.sub(r'\\b'+re.escape(var)+r'\\b', val, new_text, count=1)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# The original natural language question (with variable placeholders) is processed into a standard sample format that \n",
    "# can be used for model training, suitable for classification tasks (Classification), generation tasks (Generation) and LLM Prompting.\n",
    "# this way fills real value of each variables and make it suitable for all tasks\n",
    "\n",
    "def process_sentence(text, variables, template_id=None, template_sql=None):\n",
    "    full_text = replace_placeholders(text, variables)\n",
    "    doc = nlp(full_text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    tags = [\"O\"] * len(tokens)\n",
    "\n",
    "    for var_name, var_value in variables.items():\n",
    "        val_tokens = [t.text for t in nlp(var_value)]\n",
    "        for i in range(len(tokens) - len(val_tokens) + 1):\n",
    "            if tokens[i:i+len(val_tokens)] == val_tokens:\n",
    "                tags[i:i+len(val_tokens)] = [var_name]*len(val_tokens)\n",
    "                break\n",
    "\n",
    "    encoding = tokenizer(full_text, add_special_tokens=True)\n",
    "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "    full_sql = template_sql\n",
    "    if template_sql:\n",
    "        for var_name, val in variables.items():\n",
    "            pattern = re.compile(r'(?<=\\\")'+re.escape(var_name)+r'(?=\\\")')\n",
    "            full_sql = pattern.sub(val, full_sql)\n",
    "\n",
    "    return {\n",
    "        \"text\": full_text,\n",
    "        \"tokens\": tokens,\n",
    "        \"tags\": tags,\n",
    "        \"template_id\": template_id,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"sql\": full_sql\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e1b8d-cc82-42f2-8d54-2cf71f629c61",
   "metadata": {},
   "source": [
    "## 4. build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db782d8-5bb1-45ce-ad17-288969bde1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of classificaiton datasets: 3993 446 373\n",
      "size of generation datasets: 4347 486 447\n"
     ]
    }
   ],
   "source": [
    "classification_train, classification_dev, classification_test = [], [], []\n",
    "generation_train, generation_dev, generation_test = [], [], []\n",
    "\n",
    "for entry in data:\n",
    "    shortest_query = pick_shortest_sql(entry[\"sql\"])\n",
    "    template_in_train = shortest_query in template_sql_to_id\n",
    "    template_id = template_sql_to_id.get(shortest_query)\n",
    "    template_sql = shortest_query\n",
    "\n",
    "    for sent in entry[\"sentences\"]:\n",
    "        qsplit = sent[\"question-split\"]\n",
    "        sample = process_sentence(\n",
    "            sent[\"text\"], sent[\"variables\"],\n",
    "            template_id if template_in_train else None,\n",
    "            template_sql\n",
    "        )\n",
    "\n",
    "        if sample[\"sql\"]:  # if there's no sql, it will affect the result\n",
    "            {\"train\": generation_train,\n",
    "             \"dev\": generation_dev,\n",
    "             \"test\": generation_test}[qsplit].append(sample)\n",
    "\n",
    "        if template_in_train:\n",
    "            {\"train\": classification_train,\n",
    "             \"dev\": classification_dev,\n",
    "             \"test\": classification_test}[qsplit].append(sample)\n",
    "\n",
    "print(\"size of classificaiton datasets:\",\n",
    "      len(classification_train), len(classification_dev), len(classification_test))\n",
    "print(\"size of generation datasets:\",\n",
    "      len(generation_train), len(generation_dev), len(generation_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177577d9-b484-471d-b737-c230cd3f9233",
   "metadata": {},
   "source": [
    "## 5. save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39a5ab8-4b91-45c0-9b9b-9bee7daa2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_jsonl(filename, data_list):\n",
    "    if data_list:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for item in data_list:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def save_generation_jsonl(filename, data_list):\n",
    "    if data_list:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for item in data_list:\n",
    "                if item[\"sql\"]:\n",
    "                    pair = {\"input\": item[\"text\"], \"output\": item[\"sql\"]}\n",
    "                    f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def save_json(filename, obj):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "save_jsonl(\"./datasets/classification_train.jsonl\", classification_train)\n",
    "save_jsonl(\"./datasets/classification_dev.jsonl\", classification_dev)\n",
    "save_jsonl(\"./datasets/classification_test.jsonl\", classification_test)\n",
    "\n",
    "save_generation_jsonl(\"./datasets/generation_train.jsonl\", generation_train)\n",
    "save_generation_jsonl(\"./datasets/generation_dev.jsonl\", generation_dev)\n",
    "save_generation_jsonl(\"./datasets/generation_test.jsonl\", generation_test)\n",
    "\n",
    "save_json(\"./datasets/templates.json\", templates)\n",
    "save_json(\"./datasets/default_values.json\", default_values)\n",
    "save_json(\"./datasets/tags_vocab.json\", tags_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd5ebe-6c58-45f1-b92e-9646c68c28e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wen)",
   "language": "python",
   "name": "wen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
